{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f923831a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43622f9",
   "metadata": {},
   "source": [
    "# The Cooking Chef Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038520fa",
   "metadata": {},
   "source": [
    "The following class define the environment of the cooking chef problem, in this world there are the following elements:\n",
    "-  representation of the state define the location of the agent and if it has an eggbeater in hand with a boolean variable ass follow: (row, col, has_beater)\n",
    "-  walls: from (row,col) to : (row, col)\n",
    "-  beater location (row, col)\n",
    "-  ends (row, col)\n",
    "\n",
    "For each reached cell (identified by row and col indexes) it is assigned a reward value, to avoid the agent to waste time  it is assigned also a penalty as follow:\n",
    "- for each cell reward = -1\n",
    "- if a cell is the location of the beater and the agent hasn't yes ony in the hands the reward is +10\n",
    "- if a cell is the location of the END and the agent have a beater in hands the reward is +100\n",
    "\n",
    "The agent can perform 4 action:\n",
    "\n",
    "- MOVE LEFT = \"l\"\n",
    "- MOVE RIGHT = \"r\"\n",
    "- MOVE UP = \"u\"\n",
    "- MOVE DOWN = \"d\"\n",
    "\n",
    "\n",
    "\n",
    "Note: if the agent reach the location of the beater the state change as he automatically pick-up the beater e.g. state = (row,col, False) action = \"u\" -> (row+1,col, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5b30b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class world:\n",
    "    def __init__ (self, row, col, walls, beater, ends):\n",
    "        self.row = row\n",
    "        self.col = col\n",
    "        self.walls = walls\n",
    "        self.beater = beater\n",
    "        self.ends = ends\n",
    "        \n",
    "        \"\"\"rewards dictionary row, col, has_beater\"\"\"\n",
    "        self.reward = {}\n",
    "        \"\"\"negative rewards for the cells\"\"\"\n",
    "        for i in range(row):\n",
    "            for j in range(col):\n",
    "                self.reward[(i,j,False)]=-1\n",
    "                self.reward[(i,j,True)]=-1\n",
    "        \"\"\"overwrite rewards for the beater pick-up\"\"\"\n",
    "        for b in beater:\n",
    "            (row, col) = b\n",
    "            self.reward [row, col, False]=10\n",
    "            self.reward [row, col, True]=-1\n",
    "        \"\"\"overwrite rewards for the reached ends\"\"\"\n",
    "        for e in ends:\n",
    "            (row, col) = e\n",
    "            self.reward [row, col, True] = 50\n",
    "            self.reward [row, col, False] = -1\n",
    "    \n",
    "    \n",
    "    \"\"\"return value from the rewards dictionary\"\"\"\n",
    "    def get_reward (self, state):\n",
    "        return self.reward[state]\n",
    "    def reached_end (self, state):\n",
    "        for s in self.ends:\n",
    "            if state == s+(True,):\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "            \n",
    "            \n",
    "    \"\"\"next state: this function take as input the actual state and an action\n",
    "    and return the resulting state \"\"\"\n",
    "    def next_state (self, actual_state, action):\n",
    "        (row, col, has_beater) = actual_state\n",
    "        \n",
    "    # MOVE LEFT\n",
    "        if action == \"l\":\n",
    "            col = col-1\n",
    "            if col < 0 or ((row,col+1),(row,col)) in self.walls:\n",
    "                return actual_state\n",
    "            elif has_beater or (row, col) in self.beater:\n",
    "                new_state = (row, col, True) \n",
    "                return new_state\n",
    "            else: \n",
    "                new_state = (row, col, False) \n",
    "                return new_state\n",
    "            \n",
    "    # MOVE RIGHT\n",
    "        if action == \"r\":\n",
    "            col += 1\n",
    "            if col > self.col-1 or ((row,col-1),(row,col)) in self.walls :\n",
    "                return actual_state\n",
    "            elif has_beater or (row, col) in self.beater:\n",
    "                new_state = (row, col, True) \n",
    "                return new_state\n",
    "            else: \n",
    "                new_state = (row, col, False) \n",
    "                return new_state\n",
    "            \n",
    "    # MOVE UP\n",
    "        if action == \"u\":\n",
    "            row += 1\n",
    "            if row > self.row-1 or ((row-1,col),(row,col)) in self.walls :\n",
    "                return actual_state\n",
    "            elif has_beater or (row, col) in self.beater:\n",
    "                new_state = (row, col, True) \n",
    "                return new_state\n",
    "            else: \n",
    "                new_state = (row, col, False) \n",
    "                return new_state\n",
    "            \n",
    "    # MOVE DOWN\n",
    "        if action == \"d\":\n",
    "            row -= 1\n",
    "            if row < 0 or ((row+1,col),(row,col)) in self.walls :\n",
    "                return actual_state\n",
    "            elif has_beater or (row, col) in self.beater:\n",
    "                new_state = (row, col, True) \n",
    "                return new_state\n",
    "            else: \n",
    "                new_state = (row, col, False) \n",
    "                return new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "179e0739",
   "metadata": {},
   "outputs": [],
   "source": [
    "class agent:\n",
    "        def __init__(self, start, actions, world, discount, learning_rate, epsilon, lamb, adaptive_epsilon):\n",
    "            self.actions = actions\n",
    "            self.world = world\n",
    "            self.qtable = {}\n",
    "            self.eligibility = {}\n",
    "            for i in range(world.row):\n",
    "                for j in range(world.col):\n",
    "                    for k in [False, True]:\n",
    "                        for a in self.actions:\n",
    "                            self.qtable[(i, j, k, a)] = 0\n",
    "                            self.eligibility[(i, j, k, a)] = 0\n",
    "            self.start = start\n",
    "            self.state = start\n",
    "            self.discount = discount\n",
    "            self.learning_rate = learning_rate\n",
    "            self.epsilon = epsilon\n",
    "            self.lamb = lamb\n",
    "            self.adaptive_epsilon = adaptive_epsilon\n",
    "        \n",
    "        def select_action(self):\n",
    "            if (np.random.uniform(0, 1) < self.epsilon):\n",
    "                action = np.random.choice(self.actions)\n",
    "            else:\n",
    "                action = self.max_action(self.state)\n",
    "            return action\n",
    "\n",
    "        def max_action(self, state):\n",
    "            action = np.random.choice(self.actions)\n",
    "            maxq = self.qtable[state+(action,)]\n",
    "            for a in self.actions:\n",
    "                q = self.qtable[state+(a,)]\n",
    "                if q > maxq:\n",
    "                    maxq = q\n",
    "                    action = a\n",
    "            return action\n",
    "    \n",
    "        def take_action(self, action):\n",
    "            next_state = self.world.next_state(self.state, action)\n",
    "            next_action = self.max_action(next_state)\n",
    "            delta = (self.world.get_reward(next_state) + self.discount *self.qtable[next_state+(next_action,)]) - self.qtable[self.state+(action,)]\n",
    "            self.eligibility[self.state+(action,)]+=1\n",
    "            for i in range(self.world.row):\n",
    "                for j in range(self.world.col):\n",
    "                    for k in [False, True]:\n",
    "                        for a in self.actions:\n",
    "                            self.qtable[(i, j, k, a)] += self.learning_rate * delta * self.eligibility[(i,j,k,a)]\n",
    "                            self.eligibility[(i, j, k, a)] = self.eligibility[(i, j, k, a)] * self.discount * self.lamb\n",
    "            self.state = next_state\n",
    "        \n",
    "        def update_epsilon(self,episode):\n",
    "                self.epsilon = 1/episode\n",
    "\n",
    "        def play(self, episodes):\n",
    "            current_ep = 1\n",
    "            while current_ep <= episodes:\n",
    "                #print(self.state)\n",
    "\n",
    "                if self.adaptive_epsilon:\n",
    "                    self.update_epsilon(current_ep)\n",
    "\n",
    "                if self.world.reached_end(self.state):\n",
    "                    self.state = self.getRandomStart()\n",
    "                    #print(\"Episode\",current_ep,\"Finished...\",sep=\" \")\n",
    "                    current_ep += 1\n",
    "                else:\n",
    "                    action = self.select_action()\n",
    "                    self.take_action(action)\n",
    "\n",
    "        def getRandomStart(self):\n",
    "            row = random.choice(range(self.world.row))\n",
    "            col = random.choice(range(self.world.col))\n",
    "\n",
    "            if (row, col) in [(0,4), (2, 4), (3,4)]:\n",
    "                i = random.choice([-1, 1])\n",
    "\n",
    "                return (row, col + i, False)\n",
    "            if (row, col) in beater:\n",
    "                return (row, col, True)\n",
    "            else:\n",
    "                return (row, col, False)\n",
    "\n",
    "\n",
    "        def get_policy_sequence(self,start):\n",
    "            state = start\n",
    "            a = []\n",
    "            while(not(self.world.reached_end(state))):\n",
    "                action = self.max_action(state)\n",
    "                a.append(action)\n",
    "                state = self.world.next_state(state, action)\n",
    "            return a\n",
    "\n",
    "        def print_Q_table(self):\n",
    "            print(self.qtable)\n",
    "\n",
    "        def print_policy(self, start):\n",
    "            print(\"Policy from start\")\n",
    "            state = start\n",
    "            while(not(self.world.reached_end(state))):\n",
    "                action = self.max_action(state)\n",
    "                print(action, end=\" -> \")\n",
    "                state = self.world.next_state(state, action)\n",
    "            print(\"X\")\n",
    "\n",
    "        def print_policy_grid(self):\n",
    "            print(\"Policy for the grid\")\n",
    "            for r in range(self.world.row):\n",
    "                for c in range(self.world.col):\n",
    "                    for has_beater in [False, True]:\n",
    "                        state = (r, c, has_beater)\n",
    "                        action = self.max_action(state)\n",
    "                        print(state, action)\n",
    "\n",
    "        def convert_position(s):\n",
    "            try:\n",
    "                row,col = map(int,s.split(\",\"))\n",
    "                return (row,col)\n",
    "            except:\n",
    "                raise argparse.ArgumentTypeError(\"arguments must be row,col\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8031f4",
   "metadata": {},
   "source": [
    "# World and Agent Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8c10854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy from start\n",
      "d -> l -> l -> l -> l -> l -> l -> u -> d -> r -> r -> r -> u -> l -> u -> l -> l -> X\n",
      "Policy for the grid\n",
      "(0, 0, False) r\n",
      "(0, 0, True) r\n",
      "(0, 1, False) r\n",
      "(0, 1, True) r\n",
      "(0, 2, False) r\n",
      "(0, 2, True) r\n",
      "(0, 3, False) u\n",
      "(0, 3, True) u\n",
      "(0, 4, False) r\n",
      "(0, 4, True) u\n",
      "(0, 5, False) u\n",
      "(0, 5, True) r\n",
      "(0, 6, False) u\n",
      "(0, 6, True) u\n",
      "(0, 7, False) l\n",
      "(0, 7, True) l\n",
      "(0, 8, False) l\n",
      "(0, 8, True) l\n",
      "(1, 0, False) u\n",
      "(1, 0, True) r\n",
      "(1, 1, False) l\n",
      "(1, 1, True) r\n",
      "(1, 2, False) l\n",
      "(1, 2, True) r\n",
      "(1, 3, False) l\n",
      "(1, 3, True) u\n",
      "(1, 4, False) l\n",
      "(1, 4, True) l\n",
      "(1, 5, False) l\n",
      "(1, 5, True) l\n",
      "(1, 6, False) l\n",
      "(1, 6, True) l\n",
      "(1, 7, False) u\n",
      "(1, 7, True) r\n",
      "(1, 8, False) l\n",
      "(1, 8, True) d\n",
      "(2, 0, False) u\n",
      "(2, 0, True) d\n",
      "(2, 1, False) r\n",
      "(2, 1, True) u\n",
      "(2, 2, False) r\n",
      "(2, 2, True) u\n",
      "(2, 3, False) d\n",
      "(2, 3, True) l\n",
      "(2, 4, False) d\n",
      "(2, 4, True) l\n",
      "(2, 5, False) d\n",
      "(2, 5, True) r\n",
      "(2, 6, False) d\n",
      "(2, 6, True) d\n",
      "(2, 7, False) d\n",
      "(2, 7, True) r\n",
      "(2, 8, False) l\n",
      "(2, 8, True) d\n",
      "(3, 0, False) r\n",
      "(3, 0, True) l\n",
      "(3, 1, False) d\n",
      "(3, 1, True) l\n",
      "(3, 2, False) r\n",
      "(3, 2, True) l\n",
      "(3, 3, False) d\n",
      "(3, 3, True) d\n",
      "(3, 4, False) r\n",
      "(3, 4, True) r\n",
      "(3, 5, False) d\n",
      "(3, 5, True) d\n",
      "(3, 6, False) d\n",
      "(3, 6, True) l\n",
      "(3, 7, False) l\n",
      "(3, 7, True) l\n",
      "(3, 8, False) l\n",
      "(3, 8, True) u\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    walls = [\n",
    "        ((0,0),(1,0)),((1,0),(0,0)),\n",
    "        ((0,1),(1,1)),((1,1),(0,1)),\n",
    "        ((0,2),(1,2)),((1,2),(0,2)),\n",
    "        ((0,3),(0,4)),((0,4),(0,3)),\n",
    "        ((0,4),(0,5)),((0,5),(0,4)),\n",
    "        ((0,7),(1,7)),((1,7),(0,7)),\n",
    "        ((1,1),(2,1)),((2,1),(1,1)),\n",
    "        ((1,2),(2,2)),((2,2),(1,2)),\n",
    "        ((1,4),(2,4)),((2,4),(1,4)),\n",
    "        ((1,6),(1,7)),((1,7),(1,6)),\n",
    "        ((2,0),(2,1)),((2,1),(2,0)),\n",
    "        ((2,3),(2,4)),((2,4),(2,3)),\n",
    "        ((2,4),(2,5)),((2,5),(2,4)),\n",
    "        ((2,6),(2,7)),((2,7),(2,6)),\n",
    "        ((2,7),(3,7)),((3,7),(2,7)),\n",
    "        ((2,8),(3,8)),((3,8),(2,8)),\n",
    "        ((3,3),(3,4)),((3,4),(3,3)),\n",
    "        ((3,4),(3,5)),((3,5),(3,4)),\n",
    "        ((2,0),(3,0)),((3,0),(2,0)),\n",
    "        ((0,4),(1,4)),((1,4),(0,4))]\n",
    "    #Arguments for world and agent\n",
    "    \n",
    "\n",
    "    \n",
    "    start = (random.randint(0, 3),random.randint(0, 8), False)\n",
    "    rows = 4\n",
    "    cols = 9\n",
    "    beater = [(2,0), (2,7)]\n",
    "    ends = [(3,0)]\n",
    "\n",
    "    w = world(rows, cols, walls, beater, ends)\n",
    "    \n",
    "    actions = [\"u\", \"d\", \"l\", \"r\"]\n",
    "    discount = 0.9\n",
    "    learning_rate = 0.1\n",
    "    epsilon = 0.2\n",
    "    number_of_episodes = 1000\n",
    "    lamb = 0.5\n",
    "    adaptive_epsilon = False\n",
    "\n",
    "    a = agent(start, actions, w, discount, learning_rate, epsilon, lamb, adaptive_epsilon)\n",
    "    \n",
    "    a.play(number_of_episodes)\n",
    "    \n",
    "    #a.printQtable()\n",
    "    \n",
    "    if True:\n",
    "        a.print_policy(start)\n",
    "    \n",
    "    if True:\n",
    "        a.print_policy_grid()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
